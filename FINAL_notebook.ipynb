{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import urllib.robotparser\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA COLLECTION\n",
    "\n",
    "## MOVIE\n",
    "# First scrape daily box office info for all movies in theaters from boxofficemojo.com\n",
    "\n",
    "# Build date for URL and make requests for each day from 2015-2019\n",
    "month_and_days = {'01': [x for x in range(1,32)], '02': [x for x in range(1,29)], '03':[x for x in range(1,32)],\n",
    "                  '04':[x for x in range(1,31)],'05':[x for x in range(1,32)], '06':[x for x in range(1,31)], \n",
    "                 '07':[x for x in range(1,32)], '08':[x for x in range(1,32)], '09':[x for x in range(1,31)],\n",
    "                 '10': [x for x in range(1,32)], '11':[x for x in range(1,31)], '12':[x for x in range(1,32)]}\n",
    "\n",
    "leap_years = [2008, 2016, 2020]\n",
    "leap_year_month_days = {'01': [x for x in range(1,32)], '02': [x for x in range(1,30)], '03':[x for x in range(1,32)],\n",
    "                  '04':[x for x in range(1,31)],'05':[x for x in range(1,32)], '06':[x for x in range(1,31)], \n",
    "                 '07':[x for x in range(1,32)], '08':[x for x in range(1,32)], '09':[x for x in range(1,31)],\n",
    "                 '10': [x for x in range(1,32)], '11':[x for x in range(1,31)], '12':[x for x in range(1,32)]}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "    # build url for each year-month-day and scrape table data\n",
    "    url = 'https://www.boxofficemojo.com/date/'\n",
    "    \n",
    "    if year in leap_years:\n",
    "        m_d = leap_year_month_days\n",
    "    else:\n",
    "        m_d = month_and_days\n",
    "        \n",
    "    y = str(year)\n",
    "    \n",
    "    for month in m_d:\n",
    "        m = '-' + month\n",
    "        for day in m_d[month]:\n",
    "            if len(str(day)) == 1:\n",
    "                d = '-' + '0' + str(day)\n",
    "            else:\n",
    "                d = '-' + str(day)\n",
    "            \n",
    "            date = y + m + d\n",
    "            url_curr = url + date\n",
    "            \n",
    "            #get data\n",
    "            page = requests.get(url_curr)\n",
    "            soup = bsoup(page.text, 'lxml')\n",
    "            \n",
    "            all_divs = soup.main.find_all(\"div\", id=\"table\")\n",
    "            \n",
    "            try:\n",
    "                all_tables = all_divs[0].find_all(\"table\")\n",
    "            except:\n",
    "                print(date)\n",
    "            \n",
    "            try:\n",
    "                all_trs = all_tables[0].find_all(\"tr\")\n",
    "            except:\n",
    "                print(date)\n",
    "            \n",
    "            for i in range(1, len(all_trs)):\n",
    "\n",
    "                all_tds = all_trs[i].find_all(\"td\")\n",
    "                a_href = all_tds[2].find('a',href=True)\n",
    "                a_href = 'http://boxofficemojo.com' + a_href['href']\n",
    "                entry = []\n",
    "\n",
    "                for j in range(0, len(all_tds)):\n",
    "                    entry.append(all_tds[j].text)       \n",
    "                \n",
    "                entry = entry[0:11]\n",
    "                entry.append(a_href)\n",
    "                entry.append(date)\n",
    "\n",
    "                # append row data to all_data list\n",
    "                all_data.append(entry)\n",
    "    print(f'{year} processed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame from all_data and save to pickle\n",
    "column_names = ['TD', 'YD', 'Movie', 'Daily', '%YD', '%LW', 'Theaters', 'Avg', 'To_Date', \n",
    "                'Days', 'Distributor', 'href','Date']\n",
    "df = pd.DataFrame(all_data, columns=column_names)\n",
    "df.to_pickle('boxofficemojo-daily_gross.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data back in\n",
    "df = pd.read_pickle('boxofficemojo-daily_gross.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of unique titles\n",
    "titles_unique = list(set(df['Movie']))\n",
    "len(titles_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to each movie page to scrape genre data and add to dataframe\n",
    "genre_dict = {}\n",
    "\n",
    "for m in titles_unique:\n",
    "    h = list(df.loc[df['Movie'] == m, 'href'])[0]\n",
    "    movie_page = requests.get(h)\n",
    "    movie_soup = bsoup(movie_page.text, 'lxml')\n",
    "\n",
    "    table_div = movie_soup.find('div', {'class': 'mojo-summary-values'})\n",
    "    sub_divs = table_div.find_all('div')\n",
    "\n",
    "    for s in sub_divs:\n",
    "        if re.search('^Genres',s.text):\n",
    "            genres = s.text\n",
    "            genres = re.sub('Genres', '', genres)\n",
    "            genres = re.sub('\\s+', ' ', genres)\n",
    "    genre_dict[m] = genres.split()\n",
    "\n",
    "genre_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each movie in the dictionary, unpack genres into string and add to genre column for that film\n",
    "for m in genre_dict:\n",
    "    genres = genre_dict[m]\n",
    "    genre_str = ''\n",
    "    for g in genres:\n",
    "        genre_str += g + '|'\n",
    "    df.loc[df['Movie'] == m,'genre'] = genre_str\n",
    "\n",
    "df.genre.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the genres into their own columns with True/False values\n",
    "unique_genre =  df.genre.str.split('|').sum()\n",
    "unique_genre = set(unique_genre)\n",
    "unique_genre\n",
    "\n",
    "for g in unique_genre:\n",
    "    df[g] = df.genre.map( lambda x: g in x.split('|') )\n",
    "    \n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-numeric values for numeric data columns\n",
    "df['Daily'] = df['Daily'].str.replace('[^0-9]', '')\n",
    "df['To_Date'] = df['To_Date'].str.replace('[^0-9]', '')\n",
    "df['%YD'] = df['%YD'].str.replace('+', '')\n",
    "df['%YD'] = df['%YD'].str.replace('-', '')\n",
    "df['%YD'] = df['%YD'].str.replace('%', '')\n",
    "df['%YD'] = df['%YD'].str.replace(',', '')\n",
    "df['%YD'] = df['%YD'].str.replace('<0.1', '0')\n",
    "df['%LW'] = df['%LW'].str.replace('+', '')\n",
    "df['%LW'] = df['%LW'].str.replace('-', '')\n",
    "df['%LW'] = df['%LW'].str.replace('%', '')\n",
    "df['%LW'] = df['%LW'].str.replace(',', '')\n",
    "df['%LW'] = df['%LW'].str.replace('<0.1', '0')\n",
    "df['Theaters'] = df['Theaters'].str.replace('[^0-9]', '')\n",
    "df['Avg'] = df['Avg'].str.replace('[^0-9]', '')\n",
    "df['Days'] = df['Days'].str.replace('[^0-9]', '')\n",
    "df['Distributor'] = df['Distributor'].str.replace('\\\\n', '')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data types\n",
    "df['Daily'] = pd.to_numeric(df['Daily'])\n",
    "df['%YD'] = pd.to_numeric(df['%YD'])\n",
    "df['%LW'] = pd.to_numeric(df['%LW'])\n",
    "df['Theaters'] = pd.to_numeric(df['Theaters'])\n",
    "df['Avg'] = pd.to_numeric(df['Avg'])\n",
    "df['To_Date'] = pd.to_numeric(df['To_Date'])\n",
    "df['Days'] = pd.to_numeric(df['Days'])\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send data to pickle\n",
    "df.to_pickle('clean-boxofficemojo-daily_gross.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LABOR STATISTICS\n",
    "\n",
    "# Scrape data from the US Bureau of Labor Statistics\n",
    "# Consumer prices increase 5.0 percent for the year ended May 2021\n",
    "url = \"https://www.bls.gov/opub/ted/2021/consumer-prices-increase-5-0-percent-for-the-year-ended-may-2021.htm\"\n",
    "page = requests.get(url)\n",
    "soup = bsoup(page.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = soup.find_all(\"table\", class_=\"regular\")\n",
    "headers = all_tables[0].find('thead').text.strip().split(\"\\n\")\n",
    "headers\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "tbody = all_tables[0].find('tbody')\n",
    "entries = tbody.find_all('tr')\n",
    "\n",
    "for i in entries:\n",
    "    x = i.text.strip().split(\"\\n\")\n",
    "    x = x[0:1] + x[2:]\n",
    "    all_rows.append(x)\n",
    "\n",
    "all_dat = pd.DataFrame(all_rows, columns=headers)\n",
    "all_dat['Month'] = pd.to_datetime(all_dat['Month'])\n",
    "all_dat = all_dat.replace('%','', regex=True)\n",
    "\n",
    "heads = all_dat.columns[1:]\n",
    "for item in heads:\n",
    "    all_dat[item] = pd.to_numeric(all_dat[item])\n",
    "    \n",
    "all_dat.to_pickle(\"final-clean-consumer_prices.pkl\")\n",
    "all_dat.to_csv(\"final-clean-consumer_prices.csv\")\n",
    "all_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real average weekly earnings down 2.2 percent from May 2020 to May 2021\n",
    "url = \"https://www.bls.gov/opub/ted/2021/real-average-weekly-earnings-down-2-2-percent-from-may-2020-to-may-2021.htm\"\n",
    "page = requests.get(url)\n",
    "soup = bsoup(page.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = soup.find_all(\"table\", class_=\"regular\")\n",
    "headers = all_tables[0].find('thead')\n",
    "heads = headers.find_all('tr')\n",
    "head = [\"Month\"] + heads[1].text.strip().split(\"\\n\")\n",
    "head\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "tbody = all_tables[0].find('tbody')\n",
    "entries = tbody.find_all('tr')\n",
    "\n",
    "for i in entries:\n",
    "    x = i.text.strip().split(\"\\n\")\n",
    "    x = x[0:1] + x[2:]\n",
    "    all_rows.append(x)\n",
    "\n",
    "all_dat = pd.DataFrame(all_rows, columns=head)\n",
    "all_dat['Month'] = pd.to_datetime(all_dat['Month'])\n",
    "all_dat = all_dat.replace('\\$','', regex=True)\n",
    "all_dat = all_dat.replace('%','', regex=True)\n",
    "\n",
    "heads = all_dat.columns[1:]\n",
    "for item in heads:\n",
    "    all_dat[item] = pd.to_numeric(all_dat[item])\n",
    "    \n",
    "all_dat.to_pickle(\"final-clean-weekly_earnings.pkl\")\n",
    "all_dat.to_csv(\"final-clean-weekly_earnings.csv\")\n",
    "all_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge Datasets\n",
    "\n",
    "with open('clean-boxofficemojo-daily_gross.pkl', \"rb\") as fh:\n",
    "  dat = pickle.load(fh)\n",
    "\n",
    "\n",
    "df = dat[['Date', 'Movie', 'Daily', 'Theaters', 'Avg',\n",
    "       'To_Date', 'Days', 'Distributor', 'genre', 'Music',\n",
    "       'Short', 'Family', 'Horror', 'Crime', 'Documentary', 'News', 'Musical',\n",
    "       'History', 'Western', 'War', 'Comedy', 'Thriller', 'Action', 'Romance',\n",
    "       'Biography', 'Sport', 'Sci-Fi', 'Drama', 'Fantasy', 'Animation',\n",
    "       'Mystery', 'Adventure', 'Film-Noir', 'Adult']]\n",
    "df = df.dropna()\n",
    "col = df.columns.tolist()\n",
    "\n",
    "# mutiply genre by daily to dispay total revenue per genre \n",
    "for i in range(9, len(col)):\n",
    "    df[col[i]] = df[col[i]]*df['Daily']\n",
    "    \n",
    "# This is the main raw movie DF\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create month column and group movie data by month\n",
    "df['Month'] = df['Date'].apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "df = df.groupby(['Month']).agg({'Music':'sum',\n",
    "                             'Short':'sum',\n",
    "                             'Family':'sum',\n",
    "                             'Horror':'sum',\n",
    "                             'Crime':'sum',\n",
    "                             'Documentary':'sum',\n",
    "                             'News':'sum',\n",
    "                             'Musical':'sum',\n",
    "                             'History':'sum',\n",
    "                             'Western':'sum',\n",
    "                             'War':'sum',\n",
    "                             'Comedy':'sum',\n",
    "                             'Thriller':'sum',\n",
    "                             'Action':'sum',\n",
    "                             'Romance':'sum',\n",
    "                             'Biography':'sum',\n",
    "                             'Sport':'sum',\n",
    "                             'Sci-Fi':'sum',\n",
    "                             'Drama':'sum',\n",
    "                             'Fantasy':'sum',\n",
    "                             'Animation':'sum',\n",
    "                             'Mystery':'sum',\n",
    "                             'Adventure':'sum',\n",
    "                             'Film-Noir':'sum',\n",
    "                             'Adult':'sum'}).round(2)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new DF Genre that has all the info from the original df \n",
    "genre = df.reset_index()\n",
    "\n",
    "# make month date time\n",
    "genre['Month'] = pd.to_datetime(genre['Month'])\n",
    "\n",
    "# read and display economic data\n",
    "cp = pd.read_pickle('final-clean-consumer_prices.pkl')\n",
    "we = pd.read_pickle('final-clean-weekly_earnings.pkl')\n",
    "print(genre.columns)\n",
    "print(cp.columns)\n",
    "print(we.columns)\n",
    "genre.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all movie and economic data, save as pkl\n",
    "alldat = pd.merge(genre, cp, how='left', left_on='Month', right_on='Month')\n",
    "alldat = pd.merge(alldat, we, how='left', left_on='Month', right_on='Month')\n",
    "\n",
    "alldat.to_pickle('alldat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './alldat.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-538fbe51d123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reload the alldat pickle and store in df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./alldat.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './alldat.pkl'"
     ]
    }
   ],
   "source": [
    "# reload the alldat pickle and store in df\n",
    "with open('alldat.pkl', \"rb\") as fh:\n",
    "  df = pickle.load(fh)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../analysis_AL/alldat.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-25399904bbc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../analysis_AL/alldat.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../analysis_AL/alldat.pkl'"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('alldat.pkl')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new data frame with only movie data to show the monthly revenue trends over time\n",
    "\n",
    "cf = df[['Month', 'Music', 'Short', 'Family', 'Horror', 'Crime', 'Documentary',\n",
    "       'News', 'Musical', 'History', 'Western', 'War', 'Comedy', 'Thriller',\n",
    "       'Action', 'Romance', 'Biography', 'Sport', 'Sci-Fi', 'Drama', 'Fantasy',\n",
    "       'Animation', 'Mystery', 'Adventure', 'Film-Noir', 'Adult']]\n",
    "\n",
    "# melt data so the genre is categorical variable and revenue is the value \n",
    "cf = pd.melt(cf, id_vars=['Month'], value_vars=['Music', 'Short', 'Family', 'Horror', 'Crime', 'Documentary',\n",
    "       'News', 'Musical', 'History', 'Western', 'War', 'Comedy', 'Thriller',\n",
    "       'Action', 'Romance', 'Biography', 'Sport', 'Sci-Fi', 'Drama', 'Fantasy',\n",
    "       'Animation', 'Mystery', 'Adventure', 'Film-Noir', 'Adult'],\n",
    "        var_name='Genre', value_name='Revenue')\n",
    "\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(cf, col=\"Genre\", col_wrap=5, size=3, height=2)\n",
    "g.map(sns.lineplot, \"Month\", \"Revenue\", )\n",
    "g.set(xticks=cf.Month[0::12])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
